{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you want to do some machine learning?\n",
    "\n",
    "To the beginner, machine learning carries a certain mystique to it. I personally thought it had to be on the level of VIKI from \"I, Robot\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/VIKI.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You come off with the impression that machine learning can pretty much solve any problem you throw at it. \n",
    "\n",
    "But machine learning isn't dark magic, and once you understand the scope of what is considered machine learning, you can start to think of the kinds of problems you may be able to take a stab at with a machine learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop won't be all encompassing -- for that we would need an entire course or two. But we will give you enough theory to get you on your feet, and enough practical experience to try your hand at a machine learning problem with the package `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this workshop, we are going to focus on a subset of machine learning techniques known as supervised learning.\n",
    "\n",
    "At the very core of machine learning is relating a series of inputs <b>Y</b> to an output variable <b>X</b>. The relationship between <b>X</b> (also called the predictors) and <b>Y</b> (also called the response variable) can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Y = f(X) + $\\epsilon$</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is essentially coming up with a model for <i>f</i>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>$\\hat{Y}$ = $\\hat{f}$(X)</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we aren't interested in the form of <i>$\\hat{f}$(X)</i> at all: we just want to obtain accurate estimates of new <i>Y</i> values given a set of <i>X</i> values. If <i>$\\hat{f}$(X)</i> were a black box and we didn't know exactly what was going on inside, we wouldn't mind too much. This is called **prediction**.\n",
    "\n",
    "In other cases, we are more interested in the form of <i>$\\hat{f}$(X)</i> itself: we want to know how changes in <i>X</i> affect the output <i>Y</i>. In this case, a black box isn't ideal, and we may be willing to sacrifice accuracy in output <i>Y</i> values if we still get a general behavior in the relationship between <i>X</i> and <i>Y</i>. This is called **inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you may be wondering... wait, wait wait. This all seems too familiar. Isn't this just linear regression? I've done problems like this before! And yes, you would be right.\n",
    "\n",
    "When you hear machine learning, just think \"fancy statistics.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fancy_statistics.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tour of scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets head to the [sklearn](https://scikit-learn.org/stable/) website and take a look around.\n",
    "\n",
    "Scikit-learn can be broken down into a number of common tasks (that don't align perfectly with the modules in the package):\n",
    "\n",
    "* classification\n",
    "* regression\n",
    "* clustering\n",
    "* dimensionality reduction\n",
    "* model selection\n",
    "* pre-processing\n",
    "\n",
    "We have already talked about classification and regression briefly, but there are modules for alternate analyses (for instance *clustering*, an unsupervised learning approach) and steps performed prior to implementing a machine learning model (e.g. *pre-processing* and *dimensionality reduction*). *Model selection* is another important step that we'll demo below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first start REALLY simple and use a simple linear regressor on a self-generated dataset with one predictor (x) and one reponse (y), introducing a few key machine learning concepts along the way.\n",
    "\n",
    "While linear regression used in a statistics settings is primarily used to describe the data, machine learning usually goes beyond description (prediction and inference) and will require a few different toolsets to achieve that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a basic model:\n",
    "\n",
    "<center><i>y = 3x + 4</i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adjustable parameters\n",
    "n = 1000\n",
    "noise = 0.20\n",
    "\n",
    "x = 100*np.random.random(size=1000)\n",
    "b0 = 4\n",
    "b1 = 3\n",
    "y = b0 + b1*x\n",
    "y = y + noise*(y.max()-y.min())*np.random.normal(size=n)\n",
    "\n",
    "plt.scatter(x, y, alpha=0.4)\n",
    "plt.xlabel('x', fontsize=16)\n",
    "plt.ylabel('y', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that may be new to you is we're going to split our original dataset in two: one portion called the **training dataset** so-called because we are going to *train* our model using these datapoints. In general, this is done randomly, and the fraction of the data allocated to the training dataset is up to you-- with the understanding that the less data in the training dataset, the worse the model.\n",
    "\n",
    "The second portion, called the **test dataset** is used to evaluate how well our model performs. In this setting, our model was blind to all datapoints in the test dataset. You can evaluate error in terms of the training dataset too, but you may be overly optimistic in your model's performance.\n",
    "\n",
    "Try a few different iterations of training-test dataset splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 3))\n",
    "\n",
    "for ax, xi, yi, title in zip(axes, [x_train, x_test], [y_train, y_test], ['Training', 'Test']):\n",
    "    ax.scatter(xi, yi, alpha=0.4)\n",
    "    ax.set_xlim([0, 100])\n",
    "    ax.set_ylim([-300, 700])\n",
    "    ax.set_xlabel('x', fontsize=14)\n",
    "    ax.set_ylabel('y', fontsize=14)\n",
    "    ax.set_title(title, fontsize=18)\n",
    "fig.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the moment you've been waiting for: some linear regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 3))\n",
    "for ax, xi, yi, title in zip(axes, [x_train, x_test], [y_train, y_test], ['Training', 'Test']):\n",
    "    ax.scatter(xi, yi, alpha=0.4)\n",
    "    ax.plot(x, b0_hat + b1_hat*x, c='k', linewidth=4)\n",
    "    ax.set_xlim([0, 100])\n",
    "    ax.set_ylim([-300, 700])\n",
    "    ax.set_xlabel('x', fontsize=14)\n",
    "    ax.set_ylabel('y', fontsize=14)\n",
    "    ax.set_title(title, fontsize=18)\n",
    "fig.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you evaluate your model? It obviously couldn't predict the outputs perfectly, as there was quite a bit of noise. You may be familiar with the $R^2$ value, which is one way, but we'll look at a few different ones. You can check out a few regression metrics in `sklearn.metrics` module.\n",
    "\n",
    "The first one we will look at is the **Mean Squared Error (MSE)**, which is the average squared residual:\n",
    "\n",
    "<center> &nbsp; </center>\n",
    "<center>$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2.$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test MSE is lower than the training MSE. This isn't always the case, but more often than not is-- because the model was based on the training dataset.\n",
    "\n",
    "It's perhaps easier to interpret the square root of the mean squared error, which has the same units as *y*. You can think of this as the average deviation between our model prediction at a given *x* value and the actual value. In this case, it looks like we deviate by ~ 60 units of y with our current model. Another way to visualize this is a *parity plot*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.scatter(y_test, yp_test, alpha=0.4)\n",
    "ax.plot(np.array([-10, 310]), np.array([-10, 310]), c='k', linewidth=4)\n",
    "ax.set_xlim([-10, 310])\n",
    "ax.set_ylim([-10, 310])\n",
    "ax.set_xlabel('y (test)', fontsize=16)\n",
    "ax.set_ylabel(r'$\\hat{y}$ (test)', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is a little more difficult to interpret because it is *unbounded*: it can theoretically take on any positive value, and we can only interpret it in the context of the original dataset. It would be nicer if we had a generalizable measure so we could distinguish a good model from a bad model with a single number. For that, we can use the $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it's your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the size of the train-test split and see how it affects your MSE values and the parameters of your model (b0 and b1). Provide the following:\n",
    "\n",
    "* A plot of training MSE and test MSE as a function of the size of the training dataset\n",
    "* A plot of b0 adn b1 as a function of the size of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that your MSE and coefficient profiles change if you rerun the analysis. Train-test splits are done randomly, so you will get different models for different train-test splits. There are multiple ways to approach this. Try out the `sklearn.model_selection` for some ideas.\n",
    "\n",
    "If you do want to try something more advanced during this exercise, try adding in a nested for loop in the above analysis so you perform repeats of each train-test splitting configuration. Then graph the average MSE and coefficient values as a function of the size of the training dataset. Your curves should look smoother, and you should be able to learn a few things about selecting an appropriate train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just barely scratched the surface of regression techniques. In this example, we stuck to a 1D predictor dataset, but you can easily try a similar approach using *multiple linear regression* using the same exact function `linear_model.LinearRegression`.\n",
    "\n",
    "If you have tried multiple linear regression on your dataset, and it isn't quite giving you the accuracy you want, you can try a host of other regression models. To name a few:\n",
    "\n",
    "* LASSO regression\n",
    "* Ridge regression\n",
    "* Neural Networks\n",
    "* Passive aggressive regression\n",
    "* Huber regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is used when the response variable *y* is qualitative. Some of these may seem a little less intuitive than the regression example shown above, but the concept is still the same. We still are trying to obtain accurate predictions of a response variable based on a set of predictors.\n",
    "\n",
    "A few examples of classification:\n",
    "\n",
    "* Astronomers need to catalogue distant objects in the sky (star, galaxy, nebula, etc) based on long-exposure CDC images\n",
    "\n",
    "* Researchers investigating the behavior of viruses in vivo need to classify motion type based on trajectory datasets collected via confocal microscopy\n",
    "\n",
    "* Researchers have collected the phenotypes of individual cells in an organism and want to classify them by cell type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a classification dataset to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "x = np.random.randint(low=0, high=50, size=(2,n))\n",
    "y2 = x[1, :] + np.random.randint(low=-10, high=10, size=n)\n",
    "cl = y2 > (440 + 10*x[0, :] - 1.4*x[0, :]**2 + 0.028*x[0, :]**3)*50/800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.scatter(x[0, :], x[1, :], c=cl, cmap='viridis', s=10)\n",
    "xm = np.linspace(0, 50, 50)\n",
    "#ax.plot(xm, (440 + 10*xm - 1.4*xm**2 + 0.028*xm**3)*50/800)\n",
    "ax.set_xlim([0,50])\n",
    "ax.set_ylim([0,50])\n",
    "ax.set_xlabel(r'$x_1$', fontsize=14)\n",
    "ax.set_ylabel(r'$x_2$', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classification approach, we want to predict the class (in this case, whether the dot is yellow or purple) based on the values of the two predictors available to us, $x_1$ and $x_2$. \n",
    "\n",
    "You can probably easily do this visually -- just draw a squiggly line such that most of the yellow are above and most of the purple are below. But how do we tell a computer to do that, or express that mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic approaches uses a local sampling technique of the training dataset to calculate probabilities. The algorithm goes like this:\n",
    "\n",
    "1. select a coordinate in ($x_1$, $x_2$) space\n",
    "2. find the $k$ nearest neighbors in the training dataset to that point\n",
    "3. calculate the fraction of yellow dots in the sample (the estimated probability that a point at the given coordinate is yellow)\n",
    "4. repeat over entire ($x_1$, $x_2$) space\n",
    "\n",
    "This approach has a tuning variable $k$ that will affect how \"smooth\" the predicted outputs are. Normally users will train several models using different $k$ values to get the highest prediction accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,5))\n",
    "h = 0.5\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "\n",
    "for ax, xi, yi, title in zip(axes, [x_train, x_test], [yp_train, yp_test], ['Training', 'Test']):\n",
    "    \n",
    "    xm = np.linspace(0, 50, 50)\n",
    "    #ax.plot(xm, (440 + 10*xm - 1.4*xm**2 + 0.028*xm**3)*50/800)\n",
    "    ax.set_xlim([0,50])\n",
    "    ax.set_ylim([0,50])\n",
    "    ax.set_xlabel(r'$x_1$', fontsize=14)\n",
    "    ax.set_ylabel(r'$x_2$', fontsize=14)\n",
    "    \n",
    "    x_min, x_max = xi[:, 0].min() - 1, xi[:, 0].max() + 3\n",
    "    y_min, y_max = xi[:, 1].min() - 1, xi[:, 1].max() + 3\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = kmodel.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    ax.scatter(xi[:, 0], xi[:, 1], c=yi, cmap='viridis', s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the regression setting, we have a number of ways to evaluate our model. At the most basic, we can calculate the fraction of correctly labelled samples, ignoring their label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this method of evaluation throws some information away: are the accuracies at which we predict yellow and purple observations the same? They aren't necessarily. \n",
    "\n",
    "If you've ever taken an epidemiology class or a related coursework, you have have heard of the sensitivity (or true positive rate) and specificity (true negative rate). These are two additional metrics. These are defined as follows:\n",
    "\n",
    "<center> &nbsp; </center>\n",
    "<center>sensitivity: $TPR = \\frac{TP}{TP + FN}$</center>\n",
    "<center> &nbsp; </center>\n",
    "<center>specificity: $TNR = \\frac{TN}{TN + FP}$</center>\n",
    "\n",
    "<center> &nbsp; </center>\n",
    "<img src=\"images/epi.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "<center> &nbsp; </center>\n",
    "\n",
    "These are specifically for binary classifiers, but there are equivalents for multi-class problems. We be using `scikit-learn`'s `classification_report` function, which reports the *precision* and *recall* values. Precision is equivalent to a positive/negative predictive value, and recall is equivalent to a sensitivity/specificity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
